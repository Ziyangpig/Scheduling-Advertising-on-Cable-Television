# -*- coding: utf-8 -*-
"""stage2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v5iY0UdOkMNKVaC24e4vXhJeQuSLOeUa
"""







import os
import pandas as pd
import pyomo.environ as pyo
from collections import defaultdict
import random
import json

# --- 1. 安装依赖 (每次会话都需要) ---
print("STEP 1: 正在安装 COIN-OR CBC 求解器和 Pyomo...")
# !apt-get install -y -qq coinor-cbc
# !pip install -q pyomo
print("✅ 环境准备就绪！")

# ==============================================================================
# 单元格 2: 【新版本】创建 Stage 2 所需的详细数据文件
# 此版本会读取您上传的 stage1_weights.csv 来确保 deal_id 一致
# ==============================================================================
import pandas as pd
import random
import os

# --- 1. 创建 'data' 目录 ---
if not os.path.exists('data'):
    os.makedirs('data')

def create_stage2_input_files_from_real_stage1():
    """根据您上传的 Stage 1 文件，创建 Stage 2 当日运行所需的详细数据文件"""
    print("\n正在创建 Stage 2 的当日详细输入文件...")

    # 读取您上传的 stage1_weights.csv 来获取 deal 列表
    try:
        df_weights_uploaded = pd.read_csv('output/stage1_weights.csv')
        # 按照 'W_d' 权重降序排序，并获取最高的250个deal
        top_250_deals = df_weights_uploaded.sort_values(by='W_d', ascending=False).head(250)
        deal_ids_from_stage1 = top_250_deals['deal_id'].unique().tolist()
        print(f"成功读取到 Stage 1 权重最高的 {len(deal_ids_from_stage1)} 个 Deal IDs")
    except FileNotFoundError:
        print("错误：无法找到 output/stage1_weights.csv。请确保您已正确上传该文件。")
        return False

    # a. 创建 deals_stage2.csv (当日待播广告的详细信息)
    ads_data = []
    # 为每个从 Stage 1 读到的 deal 创建一些基础属性
    deals_base = {
        deal_id: {
            'target_demo': random.choice(['F18-34', 'M25-54', 'P25-54']),
            'advertiser': f'ADV{random.randint(1, 5)}',
            'brand': f'Brand{random.randint(1, 10)}', # BrandA, BrandB, etc.
            'category': f'Cat{random.randint(1, 5)}'
        } for i, deal_id in enumerate(deal_ids_from_stage1)
    }

    deal_ids_list = list(deals_base.keys())

    if not deal_ids_list:
        print("警告: Stage 1 中没有有效的 deal_id，无法生成广告。")
    else:
        # 为每个 deal 创建一个广告
        for ad_counter, deal_id in enumerate(deal_ids_list, 1):
            props = deals_base[deal_id]
            ads_data.append({
                'ad_id': f'Ad{ad_counter}',
                'deal_id': deal_id,
                'length_sec': 30,
                'target_demo': props['target_demo'],
                'advertiser': props['advertiser'],
                'brand': props['brand'],
                'category': props['category']
            })

    # 随机添加一些特殊约束的广告以测试模型的完整性
    if len(ads_data) > 12:
        # ads_data[5]['is_A_pos'] = True # A-Pos约束在模型中已注释，故此处也注释掉
        # ads_data[6]['is_Z_pos'] = True # Z-Pos约束在模型中已注释，故此处也注释掉
        ads_data[7]['piggyback_with'] = f'Ad{9}'
        ads_data[8]['piggyback_with'] = f'Ad{8}'
        ads_data[10]['sandwich_with'] = f'Ad{12}'
        ads_data[11]['sandwich_with'] = f'Ad{11}'

    ads_df = pd.DataFrame(ads_data)
    # df_final_output.to_csv(output_path, index=False, encoding='utf-8-sig')
    ads_df.to_csv('data/deals_stage2.csv', index=False)
    print(" -> data/deals_stage2.csv 创建成功")

    # b. 创建 breaks_stage2.csv (当日的广告时段库存)
    breaks_data = []
    break_counter = 1
    # 当天24小时都在放，大约每小时一个 break
    current_minute = 0
    end_minute = 24 * 60

    while current_minute < end_minute:
        # 每个 break 持续 5-7 分钟不等 (增加时长以确保容量充足)
        length_seconds = random.randint(5 * 60, 7 * 60)

        breaks_data.append({
            'break_id': f'B{break_counter}',
            'length_sec': length_seconds,
            'start_minute_F_b': current_minute,
            'hour': current_minute // 60
        })

        break_counter += 1
        # 下一个 break 大约在当前 break 结束后 55-65 分钟
        current_minute += random.randint(55, 65)
    breaks_df = pd.DataFrame(breaks_data)
    breaks_df.to_csv('data/breaks_stage2.csv', index=False)
    print(" -> data/breaks_stage2.csv 创建成功")

    # c. 创建 ratings_stage2.csv (根据 xgboost.csv 最后一天的数据)
    print(" -> 正在根据 xgboost.csv 创建 ratings_stage2.csv...")
    try:
        # 1. 读取 xgboost.csv 数据
        df_xgboost = pd.read_csv('xgboost.csv')

        # 2. 找到最后一天的日期
        df_xgboost['date'] = pd.to_datetime(df_xgboost['date'])
        last_date = df_xgboost['date'].max()

        # 3. 筛选出最后一天的收视率预测数据
        df_last_day = df_xgboost[df_xgboost['date'] == last_date].copy()

        # 4. 将时间 'HH:MM' 转换为分钟数
        time_parts = df_last_day['time'].str.split(':', expand=True)
        df_last_day['minutes_from_midnight'] = time_parts[0].astype(int) * 60 + time_parts[1].astype(int)

        # 5. 为每个 break 找到最接近的收视率
        ratings_data = []
        demos = ['F18-34', 'M25-54', 'P25-54']

        # 创建一个从分钟到预测收视率的映射，并删除重复分钟的记录以确保idxmin()正常工作
        df_last_day = df_last_day.drop_duplicates(subset=['minutes_from_midnight'])
        rating_map = df_last_day.set_index('minutes_from_midnight')['y_pred']

        for index, row in breaks_df.iterrows():
            break_id = row['break_id']
            start_minute = row['start_minute_F_b']

            # 找到时间上最接近的收视率记录的索引
            # .to_series() 将Index转换为Series，使其支持 .abs() 方法
            closest_minute_index = (rating_map.index.to_series() - start_minute).abs().idxmin()
            rating_value = rating_map.loc[closest_minute_index]

            for demo in demos:
                ratings_data.append({
                    'break_id': break_id,
                    'demo_id': demo,
                    'rating': round(rating_value, 2) # 使用找到的收视率
                })

        ratings_df = pd.DataFrame(ratings_data)
        ratings_df.to_csv('data/ratings_stage2.csv', index=False)
        print(" -> data/ratings_stage2.csv 创建成功 (数据来源: xgboost.csv)")

    except FileNotFoundError:
        print("错误：无法找到 xgboost.csv。将使用随机数据生成 ratings_stage2.csv。")
        # Fallback to original random generation if file not found
        ratings_data = []
        demos = ['F18-34', 'M25-54', 'P25-54']
        for b in breaks_df['break_id']:
            for demo in demos:
                ratings_data.append({ 'break_id': b, 'demo_id': demo, 'rating': round(random.uniform(0.5, 3.0), 2) })
        ratings_df = pd.DataFrame(ratings_data)
        ratings_df.to_csv('data/ratings_stage2.csv', index=False)
        print(" -> data/ratings_stage2.csv (随机数据) 创建成功")
    # --- 容量检查 ---
    total_ad_duration = ads_df['length_sec'].sum()
    total_break_duration = breaks_df['length_sec'].sum()
    print("\n--- 容量检查 ---")
    print(f"所有广告的总时长: {total_ad_duration / 60:.2f} 分钟")
    print(f"所有时段的总容量: {total_break_duration / 60:.2f} 分钟")

    if total_ad_duration > total_break_duration:
        print("❌ 错误：广告总时长超过了可用时段的总容量。模型必定无解。请调整数据生成参数。")
        return False # 返回 False 阻止后续求解

    print("✅ 容量检查通过。")
    return True

# --- 执行文件创建 ---
if create_stage2_input_files_from_real_stage1():
    print("\n✅ Stage 2 所需的当日详细数据文件已准备就绪。")

# ==============================================================================
# 单元格 3: 【新版本】加载并预处理所有数据
# 此版本会直接读取您上传的 stage1_weights.csv 和 stage1_xdb.csv
# ==============================================================================
import pandas as pd
from collections import defaultdict

print("正在加载和预处理数据...")

# --- 1. 加载所有CSV文件 ---
try:
    df_weights = pd.read_csv('output/stage1_weights.csv')
    
    print("成功加载您上传的 Stage 1 输出文件。")
except FileNotFoundError as e:
    print(f"错误: 无法找到文件 {e.filename}。请确保您已正确上传文件到 output/ 目录。")
    # 如果文件不存在，停止执行，防止后续代码出错
    raise e

df_ads = pd.read_csv('data/deals_stage2.csv')
df_breaks = pd.read_csv('data/breaks_stage2.csv')
df_ratings = pd.read_csv('data/ratings_stage2.csv')
print("成功加载 Stage 2 的当日详细数据文件。")


# --- 2. 转换数据为Pyomo友好格式 ---
# a. 基础列表
ads_list = df_ads['ad_id'].tolist()
breaks_list = df_breaks['break_id'].tolist()
deals_list = df_weights['deal_id'].unique().tolist()
advertisers_list = df_ads['advertiser'].unique().tolist()
brands_list = df_ads['brand'].unique().tolist()
categories_list = df_ads['category'].unique().tolist()
hours_list = df_breaks['hour'].unique().tolist()

# b. 将DataFrames转换为字典以便快速查找
ads_data = df_ads.set_index('ad_id').to_dict('index')
breaks_data = df_breaks.set_index('break_id').to_dict('index')
ratings_pivot = df_ratings.pivot(index='break_id', columns='demo_id', values='rating').to_dict()
weights_data = df_weights.set_index('deal_id')['W_d'].to_dict()

# c. 补充广告数据中的权重
for ad_id, ad_props in ads_data.items():
    ads_data[ad_id]['W_d'] = weights_data.get(ad_props['deal_id'], 1.0)

# d. 预处理约束
allowed_placements = [(i, b) for i in ads_list for b in breaks_list]
separation_pairs = [('Ad1', 'Ad15', 30), ('Ad2', 'Ad20', 60)] if 'Ad20' in ads_list else []
a_pos_ads = {ad for ad, props in ads_data.items() if props.get('is_A_pos')}
z_pos_ads = {ad for ad, props in ads_data.items() if props.get('is_Z_pos')}
piggyback_pairs = {(ad, props['piggyback_with']) for ad, props in ads_data.items() if pd.notna(props.get('piggyback_with')) and ad < props['piggyback_with']}
sandwich_pairs = {(ad, props['sandwich_with']) for ad, props in ads_data.items() if pd.notna(props.get('sandwich_with')) and ad < props['sandwich_with']}

# e. 构建初始排期表 (Warm Start) - 【新逻辑：高权重Deal优先高收视率Break】
print("正在根据Deal权重和Break收视率生成初始排期表 (Warm Start)...")

# 1. 对广告按Deal权重排序
df_ads_with_weights = pd.merge(df_ads, df_weights, on='deal_id', how='left')
sorted_ads = df_ads_with_weights.sort_values(by='W_d', ascending=False)['ad_id'].tolist()

# 2. 对Break按平均收视率排序
df_ratings_avg = df_ratings.groupby('break_id')['rating'].mean().reset_index()
df_breaks_with_ratings = pd.merge(df_breaks, df_ratings_avg, on='break_id', how='left')
# 填充可能没有评级的break，避免排序失败
df_breaks_with_ratings['rating'] = df_breaks_with_ratings['rating'].fillna(0)
sorted_breaks = df_breaks_with_ratings.sort_values(by='rating', ascending=False)

# 3. 贪心算法分配广告
initial_schedule = {}
break_capacity_used = {b: 0 for b in breaks_list}
breaks_queue = sorted_breaks[['break_id', 'length_sec']].to_dict('records')

for ad_id in sorted_ads:
    ad_length = ads_data[ad_id]['length_sec']
    assigned = False
    for break_info in breaks_queue:
        b_id = break_info['break_id']
        b_len = break_info['length_sec']
        if b_len - break_capacity_used[b_id] >= ad_length:
            initial_schedule[ad_id] = b_id
            break_capacity_used[b_id] += ad_length
            assigned = True
            break
    
    if not assigned:
        available_breaks = [b for b in breaks_list if breaks_data[b]['length_sec'] - break_capacity_used[b] >= ad_length]
        if available_breaks:
            chosen_break = random.choice(available_breaks)
            initial_schedule[ad_id] = chosen_break
            break_capacity_used[chosen_break] += ad_length
        else:
            initial_schedule[ad_id] = random.choice(breaks_list)

# 确保所有广告都有初始位置
for ad_id in ads_list:
    if ad_id not in initial_schedule:
        initial_schedule[ad_id] = random.choice(breaks_list)

print("✅ 新的 Warm Start 排期表生成完毕。")

# f. 聚合数据用于模型
model_data = {
    'ads_list': ads_list, 'breaks_list': breaks_list, 'deals_list': deals_list,
    'advertisers_list': advertisers_list, 'brands_list': brands_list, 'categories_list': categories_list,
    'hours_list': hours_list, 'ads_data': ads_data, 'breaks_data': breaks_data,
    'ratings_pivot': ratings_pivot, 'initial_schedule': initial_schedule,
    'allowed_placements': allowed_placements, 'separation_pairs': separation_pairs,
    'a_pos_ads': a_pos_ads, 'z_pos_ads': z_pos_ads, 'piggyback_pairs': piggyback_pairs,
    'sandwich_pairs': sandwich_pairs
}

print("\n✅ 数据加载和预处理完成。")

# ==============================================================================
# 单元格 4: 完整版 Pyomo Stage 2 模型 (无省略)
# ==============================================================================
import pyomo.environ as pyo

def solve_stage2_full_model(data, solver, Mmove_percent=0.5, PB=0.1, PA=0.1, PV=0.1):
    """
    使用 Pyomo 构建并求解 Stage 2 的完整 MIP 模型
    PB, PA, PV 是均匀分布约束的惩罚系数
    """
    print("\n--- 正在构建 Stage 2 完整版 MIP 模型 ---")

    # --- 1. 创建模型和集合 ---
    model = pyo.ConcreteModel(name="Stage2_Full_Ad_Scheduling")

    model.ADS = pyo.Set(initialize=data['ads_list'])
    model.BREAKS = pyo.Set(initialize=data['breaks_list'], ordered=True) # ordered=True很方便
    model.DEALS = pyo.Set(initialize=data['deals_list'])
    model.ADVERTISERS = pyo.Set(initialize=data['advertisers_list'])
    model.BRANDS = pyo.Set(initialize=data['brands_list'])
    model.HOURS = pyo.Set(initialize=data['hours_list'])

    # --- 2. 定义映射关系和子集 ---
    model.ads_in_deal = {d: [a for a in data['ads_list'] if data['ads_data'][a]['deal_id'] == d] for d in data['deals_list']}
    model.ads_in_brand = {br: [a for a in data['ads_list'] if data['ads_data'][a]['brand'] == br] for br in data['brands_list']}
    model.ads_in_advertiser = {adv: [a for a in data['ads_list'] if data['ads_data'][a]['advertiser'] == adv] for adv in data['advertisers_list']}
    model.breaks_in_hour = {h: [b for b in data['breaks_list'] if data['breaks_data'][b]['hour'] == h] for h in data['hours_list']}

    # --- 3. 定义决策变量 ---
    model.x = pyo.Var(model.ADS, model.BREAKS, domain=pyo.Binary)
    model.y = pyo.Var(model.ADS, domain=pyo.Binary)

    # 辅助变量 - 品牌/广告主均匀分布 (公式 13, 14)
    model.z_kb = pyo.Var(model.BRANDS, model.BREAKS, domain=pyo.Binary) # 品牌k是否在时段b出现
    model.alpha_kb = pyo.Var(model.BRANDS, model.BREAKS, domain=pyo.NonNegativeReals) # 惩罚值
    model.v_ab = pyo.Var(model.ADVERTISERS, model.BREAKS, domain=pyo.Binary) # 广告主a是否在时段b出现
    model.beta_ab = pyo.Var(model.ADVERTISERS, model.BREAKS, domain=pyo.NonNegativeReals) # 惩罚值

    # 辅助变量 - 时间均匀分布 (公式 15)
    model.theta_dh = pyo.Var(model.DEALS, model.HOURS, domain=pyo.NonNegativeReals)
    model.gamma_dh = pyo.Var(model.DEALS, model.HOURS, domain=pyo.NonNegativeReals)

    # 辅助变量 - 其他约束
    model.u = pyo.Var([(p[0], p[1]) for p in data['separation_pairs']], domain=pyo.Binary)

    # --- 4. 定义目标函数 (公式 11) ---
    def objective_rule(model):
        allocated_ratings = sum(
            data['ads_data'][i]['W_d'] * (data['ads_data'][i]['length_sec'] / 30.0) *
            data['ratings_pivot'][data['ads_data'][i]['target_demo']][b] * model.x[i, b]
            for i in model.ADS for b in model.BREAKS
        )
        binned_penalty = sum(
            data['ads_data'][i]['W_d'] * (data['ads_data'][i]['length_sec'] / 30.0) *
            data['ratings_pivot'][data['ads_data'][i]['target_demo']][data['initial_schedule'][i]] * model.y[i]
            for i in model.ADS
        )
        brand_penalty = PB * sum(model.alpha_kb[k, b] for k in model.BRANDS for b in model.BREAKS)
        advertiser_penalty = PA * sum(model.beta_ab[a, b] for a in model.ADVERTISERS for b in model.BREAKS)
        time_penalty = PV * sum(model.gamma_dh[d, h] for d in model.DEALS for h in model.HOURS)

        return allocated_ratings - binned_penalty - brand_penalty - advertiser_penalty - time_penalty

    model.objective = pyo.Objective(rule=objective_rule, sense=pyo.maximize)
    print(" -> 目标函数已定义")

    # --- 5. 定义约束 ---
    # (12a) 每个广告有唯一去处
    def ad_placement_rule(model, i):
        return sum(model.x[i, b] for b in model.BREAKS) + model.y[i] == 1
    model.ad_placement_con = pyo.Constraint(model.ADS, rule=ad_placement_rule)

    # (12b) 时段容量
    def break_capacity_rule(model, b):
        return sum(data['ads_data'][i]['length_sec'] * model.x[i, b] for i in model.ADS) <= data['breaks_data'][b]['length_sec']
    model.break_capacity_con = pyo.Constraint(model.BREAKS, rule=break_capacity_rule)

    # (12d) 最大移动数
    Mmove = int(len(data['ads_list']) * Mmove_percent)
    def max_moves_rule(model):
        ads_staying_put = sum(model.x[i, data['initial_schedule'][i]] for i in model.ADS)
        return len(data['ads_list']) - ads_staying_put <= Mmove
    model.max_moves_con = pyo.Constraint(rule=max_moves_rule)

    # (13) 品牌均匀分布
    def brand_presence_rule(model, k, b):
        return sum(model.x[i, b] for i in model.ads_in_brand[k]) <= len(model.ads_in_brand[k]) * model.z_kb[k,b]
    model.brand_presence_con = pyo.Constraint(model.BRANDS, model.BREAKS, rule=brand_presence_rule)
    def brand_penalty_rule(model, k, b):
        if b == model.BREAKS.last(): return pyo.Constraint.Skip
        # 使用 model.BREAKS.next(b) 获取下一个时段
        penalty_value = sum(data['ads_data'][i]['W_d'] * (data['ads_data'][i]['length_sec'] / 30.0) * data['ratings_pivot'][data['ads_data'][i]['target_demo']][b] * (model.x[i,b] - 1 + model.z_kb[k, model.BREAKS.next(b)]) for i in model.ads_in_brand[k])
        return model.alpha_kb[k,b] >= penalty_value
    model.brand_penalty_con = pyo.Constraint(model.BRANDS, model.BREAKS, rule=brand_penalty_rule)

    # (15) 时间均匀分布
    def time_dist_rule_pos(model, d, h):
        total_ads_for_deal = sum(model.x[i, b] for i in model.ads_in_deal[d] for b in model.BREAKS)
        avg_ads_per_hour = total_ads_for_deal / len(model.HOURS)
        ads_in_hour = sum(model.x[i, b] for i in model.ads_in_deal[d] for b in model.breaks_in_hour[h])
        return model.theta_dh[d,h] >= ads_in_hour - avg_ads_per_hour
    model.time_dist_con_pos = pyo.Constraint(model.DEALS, model.HOURS, rule=time_dist_rule_pos)

    def time_dist_rule_neg(model, d, h):
        total_ads_for_deal = sum(model.x[i, b] for i in model.ads_in_deal[d] for b in model.BREAKS)
        avg_ads_per_hour = total_ads_for_deal / len(model.HOURS)
        ads_in_hour = sum(model.x[i, b] for i in model.ads_in_deal[d] for b in model.breaks_in_hour[h])
        return model.theta_dh[d,h] >= avg_ads_per_hour - ads_in_hour
    model.time_dist_con_neg = pyo.Constraint(model.DEALS, model.HOURS, rule=time_dist_rule_neg)

    # # (16) A/Z-position
    # def a_pos_rule(model, b):
    #     return sum(model.x[i, b] for i in data['a_pos_ads']) <= 1
    # model.a_pos_con = pyo.Constraint(model.BREAKS, rule=a_pos_rule)
    # def z_pos_rule(model, b):
    #     return sum(model.x[i, b] for i in data['z_pos_ads']) <= 1
    # model.z_pos_con = pyo.Constraint(model.BREAKS, rule=z_pos_rule)

    # (18a) 关联约束 (piggyback & sandwich)
    def association_rule(model, i, j, b):
        return model.x[i, b] == model.x[j, b]
    model.association_con = pyo.Constraint(list(data['piggyback_pairs']) + list(data['sandwich_pairs']), model.BREAKS, rule=association_rule)

    # (18b) Sandwich 中间广告
    non_az_ads = [i for i in model.ADS if i not in data['a_pos_ads'] and i not in data['z_pos_ads']]
    def sandwich_middle_ad_rule(model, i, j, b):
        return sum(model.x[k, b] for k in non_az_ads if k not in [i, j]) >= model.x[i, b]
    model.sandwich_middle_ad_con = pyo.Constraint(list(data['sandwich_pairs']), model.BREAKS, rule=sandwich_middle_ad_rule)

    print(" -> 所有约束已定义")
    print("\n--- 正在求解模型 (这可能需要一些时间) ---")

    # --- 6. 求解 ---
    results = solver.solve(model, tee=True)

    return model, results

# ==============================================================================
# 单元格 5: 执行求解并展示最终结果 (已更新，增加文件输出)
# ==============================================================================
import pandas as pd
import os
from collections import defaultdict

# --- 1. 执行模型求解 ---
# 确保在运行此单元格之前，前面的单元格都已成功运行
try:
    model_data
except NameError:
    print("错误：'model_data' 未定义。请确保您已按顺序运行了前面的所有代码单元格。")
else:
    # 创建并配置求解器
    solver = pyo.SolverFactory('cbc')
  #  solver.options['seconds'] = 1800  # 增加求解时间到30分钟
    solver.options['ratio'] = 0.01  # 设置目标Gap为1%

    # 传递solver给求解函数
    model, results = solve_stage2_full_model(model_data, solver, Mmove_percent=0.7)

    # --- 2. 格式化并打印结果 ---
    print("\n--- 最终结果 ---")
    # 接受 'optimal'（最优）、'feasible'（可行）以及 'maxTimeLimit'（超时但有解）的终止条件
    # 同时放宽对求解器状态的检查，因为超时中止时状态可能是 'aborted' 或 'warning'
    if results.solver.termination_condition in [pyo.TerminationCondition.optimal, pyo.TerminationCondition.feasible, pyo.TerminationCondition.maxTimeLimit]:
        if results.solver.termination_condition == pyo.TerminationCondition.optimal:
            print(f"✅ 状态: 找到最优解")
        else:
            print(f"⚠️ 状态: 找到可行解 (可能未达到最优)")

        print(f"最终目标函数值: {pyo.value(model.objective):.2f}")

        new_schedule = defaultdict(list)
        binned_ads = []

        # 从模型中提取排期结果
        for i in model.ADS:
            if pyo.value(model.y[i]) > 0.5:
                binned_ads.append(i)
            else:
                for b in model.BREAKS:
                    if pyo.value(model.x[i, b]) > 0.5:
                        new_schedule[b].append(i)
                        break

        print("\n--- 优化后的排期方案 ---")
        # 按照时段的开始时间排序，让输出更直观
        sorted_breaks = sorted(new_schedule.keys(), key=lambda k: model_data['breaks_data'][k]['start_minute_F_b'])
        for b in sorted_breaks:
            print(f"{b} (Hour {model_data['breaks_data'][b]['hour']}): {', '.join(sorted(new_schedule[b]))}")

        if binned_ads:
            print(f"\n被放入回收站的广告: {', '.join(binned_ads)}")

        print("\n--- 传送给 Stage 3 的数据 ---")
        output_for_stage3 = {b: sorted(ads) for b, ads in new_schedule.items()}
        print(output_for_stage3)

        # 确保 output 目录存在
        output_dir = 'output'
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"创建目录: {output_dir}")

        # 准备要写入文件的数据
        output_list = []
        # 添加已成功排期的广告
        for break_id, ads_in_break in new_schedule.items():
            for ad_id in ads_in_break:
                output_list.append({'ad_id': ad_id, 'break_id': break_id, 'status': 'Scheduled'})

        # 添加被放入回收站的广告
        for ad_id in binned_ads:
            output_list.append({'ad_id': ad_id, 'break_id': 'BINNED', 'status': 'Binned'})

        # 创建 DataFrame
        if output_list:
            df_schedule_output = pd.DataFrame(output_list)

            # (可选，但推荐) 关联原始广告信息，让输出文件内容更丰富
            df_ads_info = pd.read_csv('data/deals_stage2.csv')
            df_final_output = pd.merge(df_schedule_output, df_ads_info, on='ad_id', how='left')

            # 调整列顺序并排序，方便查看
            cols_order = ['break_id', 'ad_id', 'deal_id', 'length_sec', 'target_demo', 'advertiser', 'brand', 'category', 'status']
            # 筛选出存在的列
            final_cols = [col for col in cols_order if col in df_final_output.columns]
            df_final_output = df_final_output[final_cols]
            df_final_output = df_final_output.sort_values(by=['break_id', 'ad_id']).reset_index(drop=True)

            # 保存到 CSV
            output_path = os.path.join(output_dir, 'stage2_schedule.csv')
            df_final_output.to_csv(output_path, index=False, encoding='utf-8-sig')
            print(f" -> ✅ 排期结果已保存到 {output_path}")

            # --- 保存详细求解结果到 JSON ---
            print(" -> 正在保存求解结果到 output/solver_results.json...")
            try:
                objective_value = pyo.value(model.objective)
                solver_stats = results.solver.statistics
                upper_bound = results.problem[0].upper_bound
                # 确保 objective_value 不为零以避免除零错误
                if objective_value and upper_bound and abs(objective_value) > 1e-6:
                    gap = (upper_bound - objective_value) / abs(objective_value)
                else:
                    gap = None

                results_to_save = {
                    "solver_status": str(results.solver.status),
                    "termination_condition": str(results.solver.termination_condition),
                    "objective_value": objective_value,
                    "upper_bound": upper_bound,
                    "gap": gap,
                    "enumerated_nodes": getattr(solver_stats.branch_and_bound, 'nodes', 'N/A'),
                    "total_iterations": getattr(solver_stats.branch_and_bound, 'iterations', 'N/A'),
                    "cpu_time_seconds": results.solver.time,
                    "wallclock_time_seconds": results.solver.wallclock_time,
                    "schedule": {b: sorted(ads) for b, ads in new_schedule.items()},
                    "binned_ads": sorted(binned_ads),
                }
            except (AttributeError, IndexError) as e:
                print(f" -> 警告：无法提取部分求解器统计信息 ({e})。将保存基本结果。")
                results_to_save = {
                    "solver_status": str(results.solver.status),
                    "termination_condition": str(results.solver.termination_condition),
                    "objective_value": pyo.value(model.objective),
                    "schedule": {b: sorted(ads) for b, ads in new_schedule.items()},
                    "binned_ads": sorted(binned_ads),
                }

            json_output_path = os.path.join(output_dir, 'solver_results.json')
            with open(json_output_path, 'w', encoding='utf-8') as f:
                json.dump(results_to_save, f, ensure_ascii=False, indent=4)
            print(" -> ✅ 结果已保存。")

    else:
        print(f"❌ 求解失败或未找到可行解。")
        print(f"求解器状态: {results.solver.status}")
        print(f"终止条件: {results.solver.termination_condition}")